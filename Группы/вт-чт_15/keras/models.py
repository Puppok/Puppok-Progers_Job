# 1. Модель - контейнер, содержащий все компоненты нейронки
# 2. Слои - строительные блоки нейронки
# 3. Компиляция - процесс настройки обучения нейронки
# 4. Обучение
# 5. Предсказания - запрос с новыми данными

import numpy as np
import sklearn
from keras.models import Sequential # класс для создания последовательных моделей
from keras.layers import Dense # создание полносвязных слоев

# Создание модели
# model = Sequential([
#     Dense(10, activation='relu', input_shape=(5,)),
#     Dense(1, activation='sigmoid')
# ])
#
# model.summary()

# Dense слой - полносвязный слой, где каждый нейрон связан со всеми нейронами предыдущего слоя
# units - кол-во нейронов в слое
# activation - функция активации (relu, sigmoid, tanh, softmax)
# n1 = функция(x1 * w1 + x2 * w2 + x3 * w3 ... xn * wn + bias)
    # процесс преобразования суммы входных данных
    # relu (Rectified Linear Unit) - если x>0 -> x, если x<0 -> 0
    # sigmoid - всегда от 0 до 1, используется для определения вероятностей
        # sigmoid(-x) -> 0
        # sigmoid(0) -> 0.5
        # sigmoid(x) -> 1
    # tanh - всегда от -1 до 1, используется как альтернатива relu
    # softmax - для многоклассовой классификации (преобразует числа в вероятности, сумма которых = 1)
        # [12, 54, 67]
        # [0.66, 0.1, 0.24] -> сумма = 1
# input_shape - размер входных данных (только для первого слоя)

# Dropout
# отключение части нейронов во время обучения, защита от переобучения

# Создание модели
model = Sequential([
    Dense(10, activation='relu', input_shape=(4,)),
    Dense(3, activation='softmax')
])

print(f'Структура модели: \n{model.summary()}')

print(f'Анализ модели:')
print(f'Кол-во слоев: {len(model.layers)}')
for i, layer in enumerate(model.layers):
    print(f'Слой {i + 1}.: {layer.name}\n'
          f'Тип слоя: {type(layer).__name__}\n'
          # f'Выходные данные: {layer.output_shape}\n'
          f'Кол-во параметров: {layer.count_params()}\n')

print(f'Всего параметров: {model.count_params()}')

# Слой 1 - (4 входа * 10 нейронов) + 10 bias = 50
# Слой 2 - (10 входов * 3 нейрона) + 3 bias = 33
print(f'Ожидаем: {4 * 10 + 10} + {10 * 3 + 3}')

# Компиляция модели
model.compile(
    optimizer = 'adam', # как обучаем модель
    loss = 'categorical_crossentropy', # что минимизируем
    metrics = ['accuracy'] # что отслеживаем
)

# Виды optimizer (процесс изменения весов ради уменьшения ошибки)
# adam
    # быстро обучается
    # автоматически подстраивает скорость обучения и распределение весов
# SGD
    # требует большей настройки
    # работает медленнее, но иногда с лучшими результатами
# RMSprop
    # аналог adam
    # хорош для рекуррентных сетей

# Функции потерь - измеряет, насколько сильно ответы модели отличаются от правильных
# 1. binary_crossentropy
    # для ответов "одно из двух" (да или нет, черное или белое)
    # выходной слой всегда 1 нейрон с sigmoid
# 2. categorical_crossentropy
    # для многоклассовой классификации (распознавание числа из списка, распознавание цвета)
    # выходной слой n нейронов с softmax
# 3. sparse_categorical_crossentropy
    # работает как пункт 2, но для больших объемов входных данных
# 4. mse - среднеквадратичная ошибка
    # для предсказания непрерывно меняющихся данных (курс валют, погода, биржа)
    # формула работы: (предсказание - правда)^2
    # критично относится к ошибкам
# 5. mae
    # работает аналогично mse
    # формула работы: |предсказание - правда|
    # более лояльна к ошибкам

# Метрики
# не влияют на работу и обучение модели, нужны для сбора статистики работы
# accuracy - точность
    # определяет долю верных ответов
    # accuracy = верные_ответы / все_ответы
# precision - точность положительных
    # определяет, из всех положительных ответов нейронки, сколько реально является положительными
    # используется, когда критично важны верные данные
# recall
    # определяет, из всего списка положительных ответов, сколько нашла модель
    # используется, когда нельзя пропускать положительный ответ


