# Структура нейронки в библиотеке keras
# 1. Модель - контейнер, содержащий все слои и настройки нейронки
# 2. Слои - строительный элемент нейронки, который позволяет ей думать, выполнять вычисления и тд
# 3. Компиляция - процесс настройки нейронки к обучению
# 4. Обучение
# 5. Предсказания - запрос обученной нейронке с новыми данными

from keras.models import Sequential
from keras.layers import Dense

model = Sequential([
    Dense(10, activation='relu', input_shape=(5,)),
    Dense(1, activation='sigmoid')
])

model.summary()

# Dense слой
# Принцип работы - каждый нейрон слоя взаимодействует со всеми нейронами прошлого слоя
# units - кол-во нейронов в слое
# activation - каким способом нейрон преобразует входные данные
# input_shape - размер входных данных (указывается только для первого слоя)

# Функции активаторы
# Определяют, как нейрон преобразует сумму входных данных
# n1 = relu(w1 * x1 + w2 * x2 .... w5 * x5 + bias)

    # 1. relu - relu(x) = max(0, x)
    # если отрицательное - вернет 0
    # если положительное - вернет x
    # relu(8) -> 8
    # relu(-5) -> 0
    # почти всегда используется в скрытых слоях (быстро обучается)

    # 2. sigmoid
    # всегда возвращает значение от 0 до 1
    # sigmoid(-12) -> 0
    # sigmoid(0) -> 0.5
    # sigmoid(475) -> 1
    # для бинарной классификации в выходном слое (да/нет, красный или зеленый)

    # 3. softmax
    # преобразует числа в вероятности, сумма которых равна 1
    # пример: вход - [2, 1, 0.5]
    # после softmax - [0.66, 0.24, 0.1], сумма = 1
    # используется для многоклассовой классификации (распознавание чисел (0-9), распознавание или классификация цвета)

    # 4. tanh
    # альтернативный вариант relu, но на выходе значения от -1 до 1
    # tanh(-12) -> -1
    # tanh(0) -> 0
    # tanh(475) -> 1

# Dropout (отключение нейронов)
