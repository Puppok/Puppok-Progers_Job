# Структура нейронки в библиотеке keras
# 1. Модель - контейнер, содержащий все слои и настройки нейронки
# 2. Слои - строительный элемент нейронки, который позволяет ей думать, выполнять вычисления и тд
# 3. Компиляция - процесс настройки нейронки к обучению
# 4. Обучение
# 5. Предсказания - запрос обученной нейронке с новыми данными

from keras.models import Sequential
from keras.layers import Dense
from tensorflow.python.keras.layers import Dropout

model = Sequential([
    Dense(10, activation='relu', input_shape=(5,)),
    Dense(1, activation='sigmoid')
])

model.summary() # структура модели

# Dense слой
# Принцип работы - каждый нейрон слоя взаимодействует со всеми нейронами прошлого слоя
# units - кол-во нейронов в слое
# activation - каким способом нейрон преобразует входные данные
# input_shape - размер входных данных (указывается только для первого слоя)

# Функции активаторы
# Определяют, как нейрон преобразует сумму входных данных
# n1 = relu(w1 * x1 + w2 * x2 .... w5 * x5 + bias)

    # 1. relu - relu(x) = max(0, x)
    # если отрицательное - вернет 0
    # если положительное - вернет x
    # relu(8) -> 8
    # relu(-5) -> 0
    # почти всегда используется в скрытых слоях (быстро обучается)

    # 2. sigmoid
    # всегда возвращает значение от 0 до 1
    # sigmoid(-12) -> 0
    # sigmoid(0) -> 0.5
    # sigmoid(475) -> 1
    # для бинарной классификации в выходном слое (да/нет, красный или зеленый)

    # 3. softmax
    # преобразует числа в вероятности, сумма которых равна 1
    # пример: вход - [2, 1, 0.5]
    # после softmax - [0.66, 0.24, 0.1], сумма = 1
    # используется для многоклассовой классификации (распознавание чисел (0-9), распознавание или классификация цвета)

    # 4. tanh
    # альтернативный вариант relu, но на выходе значения от -1 до 1
    # tanh(-12) -> -1
    # tanh(0) -> 0
    # tanh(475) -> 1

# Dropout - система ручного отключения какого то процента нейронов, для избегания переобучения
# переобучение - способность модели запоминать обучающие данные
# model = Sequential([
#     Dense(128, activation='relu', input_shape=(5,)),
#     Dropout(0.5),
#     Dense(60, activation='relu'),
#     Dropout(.3),
#     Dense(1, activation='sigmoid')
# ])

# Анализ модели
print(f'\nКол-во слоев: {len(model.layers)}\n'
      f'Кол-во параметров модели: {model.count_params()}\n')

for i, layer in enumerate(model.layers):
    print(f'Слой {i + 1}. {layer.name}\n'
          f'Тип слоя: {type(layer).__name__}\n'
          f'Кол-во параметров: {layer.count_params()}\n')

# Компиляция
model.compile(
    optimizer = 'adam',
    loss = 'binary_crossentropy',
    metric = ['accuracy']
)

# optimizer - как изменять веса, чтобы уменьшить вероятность ошибочного ответа
# веса нейрона - числовые коэффициенты нейрона, определяющие важность входящего значения в данный нейрон
    # adam
        # очень быстро обучается
        # учитывает историю обучения
    # SGD (Stochastic Gradient Descent)
        # работает по принципу adam
        # требует более глубокой настройки
        # медленнее обучается, но иногда дает лучшие результаты
    # RMSprop
        # аналог adam
        # хорошо работает для рекуррентных сетей

# loss - измеряет, насколько сильно ответы модели отличаются от правильных
# для классификации (предсказание класса (объекта))
    # binary_crossentropy
        # для бинарной классификации (2 класса)
    # categorical_crossentropy
        # для множества классов
    # sparce_categorical_crossentropy
        # аналогично способу выше, но для большого кол-ва входных данных
# для регресси (предсказание числового значения)
    # mse
        # для предсказаний постоянно меняющихся значений (температура, курс валют)
        # mse = (предсказание - правда)^2
    # mae
        # для предсказаний постоянно меняющихся значений (температура, курс валют)
        # mae = |предсказание - правда|

# metric - данные, показывающие статистику работы модели
    # accuracy - точность (сколько правильных ответов дала модель в ходе обучения)
        # пример: 100 итераций обучения, 80 верных ответов -> accuracy - 0.8 (80%)
    # precision - точность положительных ответов
        # из всех положительных ответов нейросети, сколько действительно является положительными
    # recall - полнота положительных ответов
        # из всех положительных ответов, сколько нашла модель
        